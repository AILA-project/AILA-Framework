{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH-4BKfu5SJh",
        "outputId": "ace4f3f5-3ab2-489c-ee99-b2d81670118e"
      },
      "outputs": [],
      "source": [
        "## Installation of libraries\n",
        "!pip install docx2txt\n",
        "!pip install --upgrade nltk\n",
        "!pip install --upgrade llama-index\n",
        "!pip install llama-index-llms-replicate\n",
        "!pip install datasets\n",
        "!pip install sentence_transformers bert-score nltk rouge-score\n",
        "!pip install llama-index-finetuning\n",
        "!pip install cohere\n",
        "!pip install llama-index-llms-cohere\n",
        "!pip install llama-index-postprocessor-cohere-rerank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPIZwKIv71kT"
      },
      "source": [
        "## Initialization of RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "4SMep2Uz5xvK",
        "outputId": "276ca787-1a9e-465e-a675-72217933e32d"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Script for creating or loading a Vector Store Index using OpenAI's GPT models and LlamaIndex.\n",
        "The index is optimized for querying large collections of legal documents.\n",
        "\n",
        "Modules:\n",
        "    os: For file and directory management.\n",
        "    openai: OpenAI API client for GPT operations.\n",
        "    nltk: Natural Language Toolkit for text preprocessing.\n",
        "    llama_index.core: Tools for creating and querying Vector Store Indexes.\n",
        "    IPython.display: Displays Markdown content in Jupyter Notebooks.\n",
        "\n",
        "Usage:\n",
        "    - Set up the working directory and API keys.\n",
        "    - Load or create a Vector Store Index from legal documents.\n",
        "    - Use the query engine to perform similarity-based searches on indexed data.\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import nltk\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from IPython.display import Markdown, display\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    StorageContext,\n",
        "    load_index_from_storage,\n",
        "    Settings\n",
        ")\n",
        "\n",
        "#### Uploading of documents to drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Set working directory and environment variables\n",
        "desired_directory = '/content/drive/MyDrive'\n",
        "os.chdir(desired_directory)\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"  # OpenAI API Key\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = \"YOUR_REPLICATE_TOKEN\"  # Replicate API token (not used here)\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')  # Sentence tokenizer\n",
        "nltk.download('stopwords')  # Common stopwords\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI()\n",
        "\n",
        "# Configure LlamaIndex settings\n",
        "Settings.chunk_size = 1024  # Maximum tokens per chunk\n",
        "Settings.chunk_overlap = 25  # Overlap between chunks for continuity\n",
        "\n",
        "# Define directory for persistent storage\n",
        "PERSIST_DIR = \"aila_indices_legislation\"\n",
        "\n",
        "# Check if an index already exists\n",
        "if not os.path.exists(PERSIST_DIR):\n",
        "    \"\"\"\n",
        "    If no index exists:\n",
        "    - Generate a sample response using GPT.\n",
        "    - Load documents from the `legislation` directory.\n",
        "    - Create and persist a Vector Store Index.\n",
        "    \"\"\"\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a legal assistant, skilled in explaining complex legal concepts with simple words.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Explain the concept of phishing.\"}\n",
        "        ]\n",
        "    )\n",
        "    print(completion.choices[0].message)  # Print the chatbot response\n",
        "\n",
        "    # Load documents and create the index\n",
        "    documents = SimpleDirectoryReader(\"legislation\").load_data()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "    # Persist the index for future use\n",
        "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "    print(\"Index created from scratch.\")\n",
        "else:\n",
        "    \"\"\"\n",
        "    If the index exists:\n",
        "    - Load it from the persistent storage directory.\n",
        "    \"\"\"\n",
        "    print(\"Loading existing index...\")\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    index = load_index_from_storage(storage_context)\n",
        "    print(\"Index loaded.\")\n",
        "\n",
        "# Configure the query engine\n",
        "query_engine = index.as_query_engine(similarity_top_k=8)\n",
        "\"\"\"\n",
        "The query engine is optimized to return the top 8 results based on similarity.\n",
        "This is suitable for retrieving the most relevant documents or passages.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSWh7q1f8Fwa"
      },
      "source": [
        "Preprocessing of *True*-False questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ttMS2jPe8FD9"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Script for processing Excel data into a Hugging Face Dataset for True/False questions.\n",
        "This script performs the following:\n",
        "1. Reads data from an Excel file.\n",
        "2. Cleans and organizes data into structured columns.\n",
        "3. Saves the processed data as a JSON file.\n",
        "4. Converts the JSON data into a Hugging Face Dataset.\n",
        "5. Splits the dataset into training and testing subsets.\n",
        "\n",
        "Modules:\n",
        "    pandas: Data manipulation and analysis.\n",
        "    json: Handles JSON file reading and writing.\n",
        "    google.colab: Facilitates file upload in Google Colab.\n",
        "    datasets: Provides tools for working with Hugging Face datasets.\n",
        "\n",
        "Usage:\n",
        "    - Ensure the Excel file \"Teliko.xls\" is in the working directory.\n",
        "    - Run the script to generate and split a Hugging Face dataset.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "from datasets import Dataset, concatenate_datasets, load_dataset\n",
        "\n",
        "# Load the Excel file\n",
        "df = pd.read_excel(\"Teliko.xls\", sheet_name='True false questions')\n",
        "\"\"\"\n",
        "Reads the 'True false questions' sheet from the Excel file into a pandas DataFrame.\n",
        "\"\"\"\n",
        "\n",
        "# Define column names for the final DataFrame\n",
        "columns = ['id', 'question', 'answer', 'metadata.justification']\n",
        "\n",
        "# Initialize lists for processing\n",
        "questions = [df.columns[1].replace('\\n', ' ').replace('\\n\\n', '')]  # Clean column header for questions\n",
        "true_false = []  # Stores answers for true/false questions\n",
        "justification = []  # Stores justifications for answers\n",
        "indexs = []  # Final list of processed records\n",
        "\n",
        "# Iterate over the rows of the DataFrame to extract and organize data\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i, 1] == 'text':  # Extract True/False answers\n",
        "        true_false.append(str(df.iloc[i, 2]).lower())\n",
        "    elif df.iloc[i, 1] == 'justification':  # Extract justification data\n",
        "        justification.append(df.iloc[i, 2])\n",
        "    elif df.iloc[i, 1] == 'rank':  # Skip rank rows\n",
        "        pass\n",
        "    else:\n",
        "        # Extract and clean question text\n",
        "        questions.append(df.iloc[i, 1].replace('\\n', ' ').replace('\\n\\n', ''))\n",
        "\n",
        "# Construct records by combining questions, answers, and justifications\n",
        "for i in range(len(questions)):\n",
        "    indexs.append([i, questions[i], true_false[i], justification[i]])\n",
        "\n",
        "# Convert the records into a pandas DataFrame\n",
        "df = pd.DataFrame(indexs, columns=columns)\n",
        "\n",
        "# Convert the DataFrame into JSON format\n",
        "json_data = df.to_json(orient='records', indent=4)\n",
        "with open('output_with_nested_metadata.json', 'w') as json_file:\n",
        "    json_file.write(json_data)\n",
        "\"\"\"\n",
        "Saves the processed data into a JSON file named 'output_with_nested_metadata.json'.\n",
        "\"\"\"\n",
        "\n",
        "# Load the JSON data from the saved file\n",
        "with open('output_with_nested_metadata.json', 'r') as json_file:\n",
        "    json_data = json.load(json_file)\n",
        "\"\"\"\n",
        "Loads the JSON file into a Python object for further processing.\n",
        "\"\"\"\n",
        "\n",
        "# Convert the JSON data into a Hugging Face Dataset\n",
        "dataset = Dataset.from_dict({\n",
        "    \"query\": [item[\"question\"] for item in json_data],\n",
        "    \"relevant_passages\": [f'{item[\"answer\"]}' for item in json_data]\n",
        "})\n",
        "\"\"\"\n",
        "Creates a Hugging Face Dataset from the processed JSON data.\n",
        "Each record consists of:\n",
        "    - `query`: The question text.\n",
        "    - `relevant_passages`: The corresponding true/false answer.\n",
        "\"\"\"\n",
        "\n",
        "# Split the dataset into training and testing subsets\n",
        "split_dataset = dataset.train_test_split(test_size=0.3, seed=0)\n",
        "train_dataset_tf = split_dataset['train']\n",
        "test_dataset_tf = split_dataset['test']\n",
        "\"\"\"\n",
        "Splits the dataset into training (70%) and testing (30%) subsets using a fixed random seed for reproducibility.\n",
        "\"\"\"\n",
        "\n",
        "# Format the training dataset for further use\n",
        "train_dataset_tf = [\n",
        "    {\"query\": query, \"relevant_passages\": [relevant_passage]}\n",
        "    for query, relevant_passage in zip(train_dataset_tf[\"query\"], train_dataset_tf[\"relevant_passages\"])\n",
        "]\n",
        "\"\"\"\n",
        "Reformats the training dataset to ensure `relevant_passages` is stored as a list for each record.\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AOrBlCS-J0M"
      },
      "source": [
        "Preprocessing of Open-Ended questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RebuPs_I9vhT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from datasets import Dataset, concatenate_datasets, load_dataset\n",
        "\n",
        "df = pd.read_excel(\"Teliko.xls\",sheet_name='open ended questions')\n",
        "\n",
        "columns = ['id','question','answer']\n",
        "questions = [df.columns[1].replace('\\n' ,' ').replace('\\n\\n','')]\n",
        "answers = []\n",
        "indexs = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i,0] == 'answer':\n",
        "        answers.append(df.iloc[i,1].replace('\\n',''))\n",
        "    elif df.iloc[i,0] == 'question ':\n",
        "        questions.append(df.iloc[i,1].replace('\\n',''))\n",
        "\n",
        "for i in range(len(questions)):\n",
        "    indexs.append([i,questions[i],answers[i]])\n",
        "\n",
        "df = pd.DataFrame(indexs,columns=columns)\n",
        "df.to_excel('phishing_open_ended_questions_new_vfinal.xlsx')\n",
        "\n",
        "json_data = df.to_json(orient='records', indent=4)\n",
        "\n",
        "with open('output_with_nested_metadata.json', 'w') as json_file:\n",
        "    json_file.write(json_data)\n",
        "\n",
        "\n",
        "# Load the JSON data from file\n",
        "with open('output_with_nested_metadata.json', 'r') as json_file:\n",
        "    json_data = json.load(json_file) # load the file as a json object\n",
        "\n",
        "dataset = Dataset.from_dict({\n",
        "    \"query\": [item[\"question\"] for item in json_data],\n",
        "    \"relevant_passages\": [f'{item[\"answer\"]}' for item in json_data]\n",
        "})\n",
        "\n",
        "\n",
        "split_dataset = dataset.train_test_split(test_size=0.3,seed=0)\n",
        "\n",
        "train_dataset_oe = split_dataset['train']\n",
        "test_dataset_oe = split_dataset['test']\n",
        "\n",
        "train_dataset_oe = [\n",
        "    {\"query\": query, \"relevant_passages\": [relevant_passage]}\n",
        "    for query, relevant_passage in zip(train_dataset_oe[\"query\"], train_dataset_oe[\"relevant_passages\"] )\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRIPetR--BfN"
      },
      "source": [
        "Preprocessing of Multiple-Choice questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "R7AJKpzT-BjL"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Script for processing Excel data into a Hugging Face Dataset for Multiple-Choice Questions.\n",
        "This script performs the following:\n",
        "1. Reads data from an Excel file containing multiple-choice questions.\n",
        "2. Cleans and organizes data into structured columns.\n",
        "3. Extracts choices, ranks, and justifications for each question.\n",
        "4. Generates JSON and Hugging Face datasets, handling nested metadata.\n",
        "5. Splits the dataset into training and testing subsets.\n",
        "\n",
        "Modules:\n",
        "    pandas: For data manipulation and analysis.\n",
        "    json: For handling JSON file reading and writing.\n",
        "    datasets: Tools for creating and managing Hugging Face datasets.\n",
        "    numpy: For numerical operations.\n",
        "    math: For mathematical utilities.\n",
        "\n",
        "Usage:\n",
        "    - Place the Excel file \"Teliko.xls\" in the working directory.\n",
        "    - Run the script to generate and split a Hugging Face dataset for multiple-choice questions.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "from datasets import Dataset, concatenate_datasets, load_dataset\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file\n",
        "df = pd.read_excel(\"Teliko.xls\", sheet_name='multiple choice questions')\n",
        "\"\"\"\n",
        "Reads the 'multiple choice questions' sheet from the Excel file into a pandas DataFrame.\n",
        "\"\"\"\n",
        "\n",
        "# Define column names for the final DataFrame\n",
        "columns = [\n",
        "    'ID', 'question', 'Choice A', 'metadata A.rank', 'metadata A.justification',\n",
        "    'Choice B', 'metadata B.rank', 'metadata B.justification',\n",
        "    'Choice C', 'metadata C.rank', 'metadata C.justification',\n",
        "    'Choice D', 'metadata D.rank', 'metadata D.justification'\n",
        "]\n",
        "\n",
        "# Initialize lists for processing\n",
        "multiple_choices = []  # Stores processed multiple-choice questions\n",
        "options = [[\"None\", \"None\", \"None\", \"None\"]]\n",
        "justifications = [[\"None\", \"None\", \"None\", \"None\"]]\n",
        "ranks = [[\"None\", \"None\", \"None\", \"None\"]]\n",
        "questions = [df.columns[1].replace('\\n', '')]  # Extract question header\n",
        "id = 0\n",
        "current_option_id = 0\n",
        "\n",
        "# Extract multiple-choice data from the DataFrame\n",
        "for i in range(len(df)):\n",
        "    if df.iloc[i, 1] in [\"text\", \"text \"]:  # Extract options\n",
        "        options[id][current_option_id] = str(df.iloc[i, 2]).replace('\\n', '')\n",
        "    elif df.iloc[i, 1] == 'justification':  # Extract justifications\n",
        "        justifications[id][current_option_id] = str(df.iloc[i, 2]).replace('\\n', '')\n",
        "    elif df.iloc[i, 1] in ['rank', 'rank ']:  # Extract ranks\n",
        "        ranks[id][current_option_id] = df.iloc[i, 2]\n",
        "        current_option_id += 1\n",
        "    elif df.iloc[i, 0] == 'question':  # Process new question\n",
        "        current_option_id = 0\n",
        "        id += 1\n",
        "        options.append([\"None\", \"None\", \"None\", \"None\"])\n",
        "        justifications.append([\"None\", \"None\", \"None\", \"None\"])\n",
        "        ranks.append([\"None\", \"None\", \"None\", \"None\"])\n",
        "        questions.append(df.iloc[i, 1].replace('\\n', ''))\n",
        "\n",
        "# Combine extracted data into structured records\n",
        "for i in range(len(questions)):\n",
        "    multiple_choices.append([\n",
        "        i, questions[i],\n",
        "        options[i][0], ranks[i][0], justifications[i][0],\n",
        "        options[i][1], ranks[i][1], justifications[i][1],\n",
        "        options[i][2], ranks[i][2], justifications[i][2],\n",
        "        options[i][3], ranks[i][3], justifications[i][3]\n",
        "    ])\n",
        "\n",
        "# Create a pandas DataFrame from the records\n",
        "df = pd.DataFrame(multiple_choices, columns=columns)\n",
        "\n",
        "# Convert the DataFrame to a dictionary for processing\n",
        "dict_data = df.to_dict(orient='records')\n",
        "\n",
        "# Process metadata and best answers for multiple-choice questions\n",
        "choices = ['A', 'B', 'C', 'D']\n",
        "for row in dict_data:\n",
        "    options_num = 0\n",
        "    row['question'] = row['question'].replace('\\n', \"\")\n",
        "    for choice in choices:\n",
        "        if pd.notna(row[f'metadata {choice}.rank']) and row[f'metadata {choice}.rank'] != \"None\":\n",
        "            if int(row[f'metadata {choice}.rank']) == 1:  # Identify the correct answer\n",
        "                answer_justification = row[f'metadata {choice}.justification'].replace('\\n', \"\").replace('\\\"', \"'\")\n",
        "                answer_choice = row[f'Choice {choice}'].replace('\\n', \"\").replace('\\\"', \"'\")\n",
        "                answer = f\"{choice}) {answer_choice}. {answer_justification}\".replace('..', '.').replace('. . ', '.')\n",
        "                row['best_answer'] = answer\n",
        "\n",
        "# Save processed data to JSON\n",
        "json_data = json.dumps(dict_data, indent=4)\n",
        "with open('output_with_nested_metadata.json', 'w') as json_file:\n",
        "    json_file.write(json_data)\n",
        "\n",
        "# Load the JSON data into a Python object\n",
        "with open('output_with_nested_metadata.json', 'r') as json_file:\n",
        "    json_data = json.load(json_file)\n",
        "\n",
        "# Create a Hugging Face Dataset from the JSON data\n",
        "dataset = Dataset.from_dict({\n",
        "    \"query\": [item[\"question\"] for item in json_data],\n",
        "    \"relevant_passages\": [item['best_answer'] for item in json_data],\n",
        "    \"Choice_A\": [str(item['Choice A']) for item in json_data],\n",
        "    \"Choice_B\": [str(item['Choice B']) for item in json_data],\n",
        "    \"Choice_C\": [str(item['Choice C']) for item in json_data],\n",
        "    \"Choice_D\": [str(item['Choice D']) for item in json_data],\n",
        "})\n",
        "\"\"\"\n",
        "Converts the JSON data into a Hugging Face Dataset.\n",
        "\"\"\"\n",
        "\n",
        "# Split the dataset into training and testing subsets\n",
        "split_dataset = dataset.train_test_split(test_size=0.3, seed=0)\n",
        "train_dataset_mc = split_dataset['train']\n",
        "test_dataset_mc = split_dataset['test']\n",
        "\"\"\"\n",
        "Splits the dataset into training (70%) and testing (30%) subsets using a fixed random seed for reproducibility.\n",
        "\"\"\"\n",
        "\n",
        "# Prepare the training dataset for further processing\n",
        "questions = []\n",
        "options = []\n",
        "for item in train_dataset_mc:\n",
        "    for row in dict_data:\n",
        "        if row['question'] == item['query']:\n",
        "            questions_row = []\n",
        "            options_row = []\n",
        "            for choice in choices:\n",
        "                if pd.notna(row[f'metadata {choice}.rank']) and row[f'metadata {choice}.rank'] != \"None\":\n",
        "                    answer_justification = row[f'metadata {choice}.justification'].replace('\\n', \"\").replace('\\\"', \"'\")\n",
        "                    answer_choice = row[f'Choice {choice}'].replace('\\n', \"\").replace('\\\"', \"'\")\n",
        "                    answer = f\"{choice}) {answer_choice}. {answer_justification}\".replace('..', '.').replace('. . ', '.')\n",
        "                    question = row['question'].replace('\\n', \"\")\n",
        "                    if int(row[f'metadata {choice}.rank']) == 1:\n",
        "                        questions_row.append(question)\n",
        "                        options_row.append(answer)\n",
        "                    else:\n",
        "                        questions_row.append(question + f\"(Not correct option {int(row[f'metadata {choice}.rank'])})\")\n",
        "                        options_row.append(answer + f\"(Not correct option {int(row[f'metadata {choice}.rank'])})\")\n",
        "            questions.append(questions_row)\n",
        "            options.append(options_row)\n",
        "\n",
        "# Structure the data into a new dictionary for saving\n",
        "dict_data = []\n",
        "for i in range(len(questions)):\n",
        "    for j in range(len(questions[i])):\n",
        "        dict_data.append({\"question\": questions[i][j], \"option\": options[i][j]})\n",
        "\n",
        "# Save the structured data back to JSON\n",
        "json_data = json.dumps(dict_data, indent=4)\n",
        "with open('output_with_nested_metadata.json', 'w') as json_file:\n",
        "    json_file.write(json_data)\n",
        "\n",
        "# Reload the JSON data for creating a Hugging Face Dataset\n",
        "with open('output_with_nested_metadata.json', 'r') as json_file:\n",
        "    json_data = json.load(json_file)\n",
        "\n",
        "dataset = Dataset.from_dict({\n",
        "    \"query\": [item[\"question\"] for item in json_data],\n",
        "    \"relevant_passages\": [item['option'] for item in json_data],\n",
        "})\n",
        "\"\"\"\n",
        "Creates a new Hugging Face Dataset with questions and options for training.\n",
        "\"\"\"\n",
        "\n",
        "# Prepare the final training dataset\n",
        "train_dataset_mc = [\n",
        "    {\"query\": query, \"relevant_passages\": [relevant_passage]}\n",
        "    for query, relevant_passage in zip(dataset[\"query\"], dataset[\"relevant_passages\"])\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpCqp5s-DZgv"
      },
      "source": [
        "Finetuning of RAG model (only run it for finetuning)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5KO3lgjDZRf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Script for fine-tuning a Cohere model for reranking and integrating it with LlamaIndex.\n",
        "This script performs the following:\n",
        "1. Loads a dataset for reranking tasks.\n",
        "2. Creates and fine-tunes a Cohere model on the dataset.\n",
        "3. Retrieves the fine-tuned model for reranking queries.\n",
        "4. Integrates the fine-tuned Cohere model with a LlamaIndex query engine.\n",
        "\n",
        "Modules:\n",
        "    cohere: Python client for the Cohere API.\n",
        "    llama_index: Provides tools for indexing and querying with advanced models.\n",
        "    os: For environment and file handling.\n",
        "\n",
        "Usage:\n",
        "    - Set the `COHERE_API_KEY` to your valid Cohere API key.\n",
        "    - Ensure the reranking dataset (`data.json`) is available in the specified path.\n",
        "    - Run the script to fine-tune the model and integrate it with LlamaIndex.\n",
        "\"\"\"\n",
        "\n",
        "import cohere\n",
        "from cohere.finetuning import FinetunedModel, Settings, BaseModel\n",
        "import os\n",
        "from llama_index.llms.cohere import Cohere\n",
        "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
        "\n",
        "# Set up Cohere API key\n",
        "COHERE_API_KEY = \"YOUR API KEY\"  # Replace with your Cohere API key\n",
        "co = cohere.Client(COHERE_API_KEY)\n",
        "\"\"\"\n",
        "Initializes the Cohere client using the provided API key.\n",
        "\"\"\"\n",
        "\n",
        "# Step 1: Create a dataset for reranking fine-tuning\n",
        "rerank_dataset = co.datasets.create(\n",
        "    name=\"rerank-dataset\",\n",
        "    data=open(\"./data.json\", \"rb\"),  # Load the dataset from the specified file\n",
        "    type=\"reranker-finetune-input\"  # Specify dataset type for reranker fine-tuning\n",
        ")\n",
        "\"\"\"\n",
        "Creates a dataset on the Cohere platform for fine-tuning a reranking model.\n",
        "The dataset file (`data.json`) must be in the expected format for reranker fine-tuning.\n",
        "\"\"\"\n",
        "\n",
        "# Wait for the dataset to be processed\n",
        "print(co.wait(rerank_dataset))\n",
        "\n",
        "# Step 2: Fine-tune a reranker model using the dataset\n",
        "finetune = co.finetuning.create_finetuned_model(\n",
        "    request=FinetunedModel(\n",
        "        name=\"reranked_model\",  # Name for the fine-tuned model\n",
        "        settings=Settings(\n",
        "            base_model=BaseModel(\n",
        "                name=\"english\",  # Base language model\n",
        "                base_type=\"BASE_TYPE_RERANK\",  # Specify reranking as the fine-tuning objective\n",
        "            ),\n",
        "            dataset_id=rerank_dataset.id,  # ID of the dataset created above\n",
        "        ),\n",
        "    )\n",
        ")\n",
        "\"\"\"\n",
        "Creates and starts the fine-tuning process for a reranker model on the specified dataset.\n",
        "\"\"\"\n",
        "\n",
        "# Step 3: Retrieve the fine-tuned model\n",
        "ft = co.finetuning.get_finetuned_model(\"finetune-model-id\")\n",
        "\"\"\"\n",
        "Retrieves the fine-tuned model using its unique model ID.\n",
        "Replace `\"finetune-model-id\"` with the actual ID of the model created.\n",
        "\"\"\"\n",
        "\n",
        "# Step 4: Set up Cohere reranker with the fine-tuned model\n",
        "cohere_rerank = CohereRerank(\n",
        "    api_key=COHERE_API_KEY,\n",
        "    model=ft.finetuned_model.id + '-ft'  # Use the fine-tuned model ID\n",
        ")\n",
        "\"\"\"\n",
        "Initializes the Cohere reranker postprocessor using the fine-tuned model.\n",
        "\"\"\"\n",
        "\n",
        "# Step 5: Integrate the reranker with a LlamaIndex query engine\n",
        "query_engine = index.as_query_engine(node_postprocessors=[cohere_rerank])\n",
        "\"\"\"\n",
        "Integrates the Cohere reranker into the LlamaIndex query engine.\n",
        "This setup applies reranking postprocessing to the query results.\n",
        "\"\"\"\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
